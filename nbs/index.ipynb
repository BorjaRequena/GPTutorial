{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTutorial\n",
    "\n",
    "> GPT: the model even your dog has heard of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial has been developed for the [VI Pyrenees winter school](https://setcases6.wordpress.com/) (Feb. 2023) of quantum information.\n",
    "\n",
    "If you're a participant of the school, please see [material](#material) below. \n",
    "\n",
    "If you're not a participant of the school, you're probably seeing this content after the actual hands-on tutorial has happened. However, you may find it useful to visit the [the tutorial page](https://borjarequena.github.io/GPTutorial/) and read through the completed [tutorial walkthrough](https://borjarequena.github.io/GPTutorial/metatutorial.html) to learn about unsupervised learning and how to train a GPT-like architecture.\n",
    "\n",
    "# Content overview\n",
    "\n",
    "First, we start introducing the basics of unsupervised learning and generative models. We explain the main concepts behind language models and how to train them, such as the tokenization process or the chain rule of probability.\n",
    "\n",
    "::: {.callout-note}\n",
    "To prepare this tutorial, we have used some of the content from our [machine learning course](https://borjarequena.github.io/Neural-Network-Course/).\n",
    ":::\n",
    "\n",
    "Then, we build a bigram language model, which allows us to introduce the concept of an embedding table and the typical training loops. This model serves both as baseline and warm up.\n",
    "\n",
    "Finally, we proceed to learn about the specific inner workings of GPT. We start by introducing the transformer architecture and the self-attention mechanism. Then, we show how to use causal self-attention (or masked self-attention) to train our model to generate new text. Finally, we dive into the details such as skip connections and layer normalization.\n",
    "\n",
    "To finish, we train a larger model that sucessfully learns to count! :)\n",
    "\n",
    "# Material\n",
    "\n",
    "We provide two main notebooks that can run in colab:\n",
    "\n",
    "- `nbs/tutorial.ipynb` is ready to be filled during the session\n",
    "- `nbs/metatutorial.ipynb` is the already complete tutorial\n",
    "\n",
    "To follow along with the tutorial, you need the first notebook. Ideally, you could simply go to [its page](https://borjarequena.github.io/GPTutorial/tutorial.html) and run it on google colab. However, since the wifi here is unstable, you can clone (or simply download) [the repository](https://github.com/BorjaRequena/GPTutorial) to your computer.\n",
    "\n",
    "If you want to follow along, make sure you have [pytorch](https://pytorch.org/) installed. We won't use any fancy stuff from pytorch so you can simply run `conda install pytorch -c pytorch` to install it with conda or `pip install torch` to install it with pip. In case you have a GPU, find your installed CUDA version (e.g. 11.7) to install `conda install pytorch-cuda=11.7 -c nvidia`. Follow their [getting started page](https://pytorch.org/get-started/locally/) in case of doubt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
