{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTutorial\n",
    "\n",
    "> GPT: the model even your dog has heard of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial has been developed for the [VI Pyrenees winter school](https://setcases6.wordpress.com/) (Feb. 2023) for quantum information.\n",
    "\n",
    "# Content overview\n",
    "\n",
    "First, we start introducing the basics of unsupervised learning and generative models. We explain the main concepts behind language models and how to train them, such as the tokenization process or the chain rule of probability.\n",
    "\n",
    "::: {.callout-note}\n",
    "To prepare this tutorial, we have used some of the content from our [machine learning course](https://borjarequena.github.io/Neural-Network-Course/).\n",
    ":::\n",
    "\n",
    "Then, we provide a brief introduction to [pytorch](https://pytorch.org/) showing how to compute gradients with automatic differentiation and how to build models. We build a bigram model as baseline to warm up.\n",
    "\n",
    "Finally, we proceed to learn about the specific inner workings of GPT. We start by introducing the transformer architecture and the self-attention mechanism. Then, we show how to use masked self-attention to train our model to generate new text. Finally, we dive into the details such as skip connections and layer normalization.\n",
    "\n",
    "::: {.callout-note}\n",
    "This is a live-coding tutorial. During the session, we go back and fourth to add layers of complexity to our models. Thus, the complete notebook may not be very informative. We recommend that you look at the MetaGPTutorial notebook, which is the guide we follow to do it during the session.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
