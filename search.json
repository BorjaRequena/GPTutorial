[
  {
    "objectID": "tutorial.html#generative-modeling",
    "href": "tutorial.html#generative-modeling",
    "title": "Tutorial",
    "section": "Generative modeling",
    "text": "Generative modeling\nIn this tutorial, we focus on generative learning. As we have briefly mentioned before, it consists on learning the data distribution to generate new samples. This is extremely powerful both on its own, since high-quality new samples can be very valuable, and in combination with other tools to tackle downstream tasks.\nThere are many data generation approaches that we can consider. The most straightforward one is to simply generate samples that are similar to the traning ones, such as face images or digits (e.g., this cat). We can also have conditioned synthesis, such as generating an audio signal from a text prompt that can be conditioned to a specific speaker voice (e.g. WaveNet). This involves all sorts of translation tasks, where we write text from a sample fragment, generate a new image from a reference one (see the emblematic horse-to-zebra example), or even create a video from a text fragment!\nThis is a very broad field and here we just show a hand full of representative examples."
  },
  {
    "objectID": "tutorial.html#example-task",
    "href": "tutorial.html#example-task",
    "title": "Tutorial",
    "section": "Example task",
    "text": "Example task\nThe goal of this tutorial is to train a GPT-like model to count numbers: “1,2,3,4,…,8765,8766,8767,…”. This seems like a rather simple task that could be easily achieved numerically with a single line of code. However, we will consider the digits as strings that conform sentences.\nThis toy example will allow us to understand the main concepts behind language models. We will use it as a running example and implement the main ideas as we see them.\nHere, we will build our data set, which is nothing more than a text document containing the numbers.\n\nmax_num = 1_000_000\ntext = \",\".join([str(i) for i in range(max_num)])\n\nLet’s see the first and last few digits of our data set.\n\n\nCode\nprint(text[:20])\nprint(text[-20:])\n\n\n0,1,2,3,4,5,6,7,8,9,\n999997,999998,999999"
  },
  {
    "objectID": "tutorial.html#giving-numerical-meaning-to-text",
    "href": "tutorial.html#giving-numerical-meaning-to-text",
    "title": "Tutorial",
    "section": "Giving numerical meaning to text",
    "text": "Giving numerical meaning to text\nWe can communicate very deep concepts with words, but how does a machine understand them?\nWhen we work with text, we split it into elementary pieces called tokens. This is known as tokenization and there is quite a lot of freedom on how to do it. For example, we can take from full sentences, to words, to single characters. The most common practice is to use sub-word tokens that are between single characters to full words, such as SentencePiece. We can also have special tokens to account for additional grammatical information. For example, we can use special tokens to indicate the beggining and ending of a sequence, or to indicate that the words start with capital letters.\nLet’s see a simple tokenization example. We would take the following sentence:\nMy cat won't stop purring.\nAnd transform it into the tokens:\n&lt;BoS&gt;&lt;x_maj&gt;&lt;my&gt; &lt;cat&gt; &lt;wo&gt;&lt;n't&gt; &lt;stop&gt; &lt;purr&gt;&lt;ing&gt;&lt;.&gt;&lt;EoS&gt;\n(I just made up this tokenization, this is just to provide an idea.)\nWith this, we define a vocabulary of tokens. To provide them with “meaning”, we assign a trainable parameter vector to each of them, which are known as embedding vectors. The larger the embedding, the richer the information we can associate to every individual token. We typically store these vectors in a so-called embedding matrix, where every row provides the associated embedding vector to a token. This way, we identify the tokens by an integer index that corresponds to their row in the embedding matrix.\nTaking long tokens results into large vocabularies and, therefore, we need more memory. However, we can generate a piece of text with just a few inference steps. Conversely, short tokens require much less memory at the cost of more inference steps to write. Thus, this presents a trade-off between memory and computational time. You can get some intuition about it by comparing the number of letters in the alphabet (shortest possible tokens) with the number of entries in a dictionary (every word is a token).\nTo process a piece of text, we first split it into the tokens of our vocabulary (tokenization), and replace the tokens by their corresponding indices (numericalization).\nLet’s see how this works in our example task. First of all, we build the token vocabulary. In this simple case, every digit is a token together with the separator “,”.\n\n# Build the vocabulary based on the text (also its size)\n\n[',', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n\nNow we can build a Tokenizer class to encode raw text into tokens, and decode tokens to actual text.\n\nclass Tokenizer:\n    def __init__(self, vocab):\n        pass\n    \n    def encode(self, string):\n        \"Tokenize an input string\"\n        pass\n    \n    def decode(self, ints):\n        \"Transform a list of integers to a string of characters\"\n        pass\n\n\ntkn = Tokenizer(vocab)\n\nLet’s see the map from tokens to integer.\n\ntkn.s2i\n\nWe can try our tokenizer with a text example.\n\n# Show an example of pre-tokenized and tokenized text\n\nWe can also test the decoding function by encoding and decoding.\n\n# Encode-decode the pre-tokenized text\n\nHere we only perform the text pre-processing. The embedding belongs to the machine learning model."
  },
  {
    "objectID": "tutorial.html#learning-the-data-probability-distribution",
    "href": "tutorial.html#learning-the-data-probability-distribution",
    "title": "Tutorial",
    "section": "Learning the data probability distribution",
    "text": "Learning the data probability distribution\nTo learn how to generate text, we need to learn the underlying distribution of the data we wish to replicate \\(p_{\\text{data}}(\\mathbf{x})\\). We model text as a sequence of tokens \\(\\mathbf{x}=\\left[x_1, x_2, \\dots, x_{T-1}\\right]\\), and the goal is to predict the next token \\(x_T\\). This way, we can recursively generate text:\n\nWe start with some initial context \\(x_1, x_2, \\dots, x_{T-1}\\).\nWe predict the next token \\(x_T\\), given the context.\nWe append the prediction to the existing text and repeat the process taking \\(x_1,\\dots,x_T\\) as context.\n\nWe typically do this defining a parametrized model to approximate the probability distribution, \\(p_\\theta(\\mathbf{x})\\approx p_{\\text{data}}(\\mathbf{x})\\). The parameters \\(\\theta\\) can represent from the weights of a neural network, to the coefficients of a gaussian mixture model.\nA standard technique in the machine learning field is to use the chain rule of probability to model sequential data. This way, the probability to observe a sequence of tokens can be described as \\[p_{\\theta}(\\mathbf{x})=p_\\theta(x_1)\\prod_{t=2}^{T}p_\\theta(x_t|x_1\\dots x_{t-1})\\,.\\]\nWe optimize our model parameters to obtain the maximum likelihood estimator, which is the most statistically efficient estimator. In this tutorial, we do not want to dive too deep in the details. The main intuition behind it is that we try to maximize the likelihood of observing the training data under our parametrized model. As such, we wish to minimize the negative log-likelihood loss or cross-entropy loss: \\[\\theta^* = \\text{arg}\\,\\text{min}_\\theta - \\frac{1}{N}\\sum_{i=1}^N \\log p_\\theta\\left(\\mathbf{x}^{(i)}\\right) = \\text{arg}\\,\\text{min}_\\theta - \\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^T \\log p_\\theta\\left(x_t^{(i)}|x_{&lt;t}^{(i)}\\right)\\]\nWe can understand the task as a classification problem at every time-step where the goal is to predict the token that follows. Thus, we can build our self-supervised classification task by simply taking the text shifted by one position as target for our prediction. For example, consider the tokenized sentence\n&lt;this&gt; &lt;language&gt; &lt;model&gt; &lt;rocks&gt;&lt;!&gt;\nGiven the tokens\n&lt;this&gt; &lt;language&gt;\nwe wish to predict\n&lt;model&gt;\namong all the tokens in the vocabulary.\nAs we typically do in machine learning, we find the optimal parameters \\(\\theta^*\\), i.e., train our model, with gradient-based optimization."
  },
  {
    "objectID": "tutorial.html#data-processing",
    "href": "tutorial.html#data-processing",
    "title": "Tutorial",
    "section": "Data processing",
    "text": "Data processing\nFirst of all, we need to properly arrange our data. We will start by tokenizing the whole text piece.\n\ndata = torch.tensor(tkn.encode(text))\ndata[:20]\n\ntensor([ 1,  0,  2,  0,  3,  0,  4,  0,  5,  0,  6,  0,  7,  0,  8,  0,  9,  0,\n        10,  0])\n\n\nNow we need to save a part of the data for validation and keep the rest for training. In generative models, we do not tend to use too much data for validation because it is just to get a rough idea of how it is working. In the end, we will evaluate the performance ourselves asking the model to generate samples.\nTo keep this simple, we will save the last numbers as validation data.\n\n\n\n\n\n\nNote\n\n\n\nGiven the nature of our data, it would be best to save chunks of the data sampled at different points along the whole text piece.\n\n\n\nval_pct = 0.1\nsplit_idx = int(len(data)*val_pct)\ndata_train = data[:-split_idx]\ndata_val = data[-split_idx:]\n\n\ndata_train.shape, data_val.shape\n\n(torch.Size([6200001]), torch.Size([688888]))\n\n\nTo train machine learning models, we take advantage of parallelization to process several samples at once. To do so, we will split the text in sub-sequences from which we will build our training batches.\n\ndef get_batch(data, batch_size, seq_len):\n    # Sample a batch of sequences of seq_len starting at random positions\n    return x.to(device), y.to(device)\n\n\nbatch_size = 64\nseq_len = 8\nxb, yb = get_batch(data_train, batch_size, seq_len)\n\n\nxb.shape\n\n\nxb[0], yb[0]"
  },
  {
    "objectID": "tutorial.html#model-definition",
    "href": "tutorial.html#model-definition",
    "title": "Tutorial",
    "section": "Model definition",
    "text": "Model definition\nWe will make a bigram model that predicts the following character based on the previous one. These models are stochastic and, therefore, the output of the model is a probability distribution over our vocabulary. We can easily achieve this by making the embedding size as large as the vocabulary. This way, when we index into the embedding matrix with a token, we immediately obtain the probability distribution over the possible next tokens.\n\nclass BigramLanguageModel(nn.Module):\n    \"Language model that predicts text based on the previous character.\"\n    def __init__(self, vocab_size):\n        super().__init__()\n        # Define the embedding\n        \n    def forward(self, x):\n        # Compute the logits\n        pass\n        \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            # Compute logits\n            # Take the last\n            # Compute the probabilities\n            # Sample the next token\n            # Append the token to the text\n        return x\n\n\nbigram_model = BigramLanguageModel(vocab_size).to(device)\n\n\nxb.shape, bigram_model(xb).shape\n\nThe logits we define here are the unnormalized probability scores for each token. To transform them in a normalized probability distribution, we use a SoftMax function. We will see below that pytorch takes the logits directly to compute the loss function instead of the probabilities.\nLet’s try generating some text with our model.\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(bigram_model.generate(context, 20)[0].tolist())"
  },
  {
    "objectID": "tutorial.html#training-loop",
    "href": "tutorial.html#training-loop",
    "title": "Tutorial",
    "section": "Training loop",
    "text": "Training loop\nWith the data and the model, we’re almost ready to do the training. We need to define a loss function and an optimiziation algorithm to update our model parameters.\nAs we have mentioned before, we wish to minimize the negative log-likelihood of the data with respect to the model. To do so, we use pytorch’s cross entropy loss.\n\ndef cross_entropy_loss(logits, targets):\n    \"Cross entropy loss flattening tensors\"\n    BS, T, H = logits.shape\n    loss = F.cross_entropy(logits.view(BS*T, H), targets.view(-1))\n    return loss\n\nThen, as optimizer, we will use Adam.\n\noptimizer = torch.optim.AdamW(bigram_model.parameters(), lr=1e-3)\n\nNow let’s define the training loop.\n\nbatch_size = 32\nseq_len = 24\ntrain_steps = 3000\n\nfor _ in range(train_steps):\n    # Sample a batch\n    # Compute the logits\n    # Compute the loss\n    # Get the gradients\n    # Make an optimization step\n    \nprint(loss.item())\n\n2.370857000350952\n\n\nWe will plug this into a function for later usage and estimate the loss on the validation set.\n\ndef train_model(steps, model, lr, batch_sz, seq_len):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for i in range(steps):\n        xb, yb = get_batch(data_train, batch_sz, seq_len)\n        logits = model(xb)\n        loss = cross_entropy_loss(logits, yb)\n        loss.backward()\n        optimizer.step()\n        if i % 200 == 0 or i == steps - 1:\n            losses = estimate_loss(model, batch_sz, seq_len)\n            print(f\"Step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    return model\n\n@torch.no_grad()\ndef estimate_loss(model, batch_sz, seq_len, eval_iters=50):\n    \"\"\"Estimate losses for train and validation data sets.\n    Adapted from https://github.com/karpathy/nanoGPT\"\"\"\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(data_train if split == 'train' else data_val,\n                             batch_sz, seq_len)\n            logits = model(X)\n            loss = cross_entropy_loss(logits, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nBigram models can’t accomplish this example task. After every digit, all the other digits are equally likely to happen if we do not consider any further context. This model can only take advantage of the separator ,. For instance, we know there will not be two consecutive separators and that the following number won’t start with 0.\nWe can see this in the first row of the embedding matrix.\n\nembedding_matrix = list(bigram_model.parameters())[0] \nembedding_matrix.softmax(-1)[0]\n\nLet’s generate some text.\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(bigram_model.generate(context, 20)[0].tolist())\n\nIn contrast to the previous example, we see the model has learned to not add consecutive separators, but the digits are still random. GPT time!"
  },
  {
    "objectID": "tutorial.html#transformer",
    "href": "tutorial.html#transformer",
    "title": "Tutorial",
    "section": "Transformer",
    "text": "Transformer\nThe architecture behind the GPT language models is based on the transformer, depicted in the figure below.\n\nThe transformer was introduced as an architecture for translation tasks with two main parts: the encoder (left) and the decoder (right). The decoder is the responsible part for generating the translated text and, thus, it is the language model bit of the whole architecture.\nThe transformer architecture relies heavily on self-attention mechanisms. Indeed, the original paper is called “Attention is all you need”. Unlike the bigram model, the transformer decoder can account for all the possible relationships between tokens in the past text to generate the new tokens."
  },
  {
    "objectID": "tutorial.html#causal-self-attention",
    "href": "tutorial.html#causal-self-attention",
    "title": "Tutorial",
    "section": "Causal self-attention",
    "text": "Causal self-attention\nThe key element in the transformer architecture is the self-attention layer. This allows our tokens in our text piece to “communicate with each other” in a fixed way:\n\nFor every token, we compute three quantities: a key \\(\\mathbf{k}\\), a query \\(\\mathbf{q}\\) and a value \\(\\mathbf{v}\\).\nThen, tokens compare their query to the other tokens’ keys.\nThe resulting value for each token is the weighted average of all the values according to the query-key similarity.\n\nWe compute the similarity between keys and queries doing the dot product between the vectors. Then, to ensure the similarity weights are normalized, we apply the softmax activation function to all the dot products of the query of interest with all the keys. We can efficiently compute all of these with matrix multiplications: \\[\\text{Attention}(Q,K,V) = \\text{SoftMax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\,\\] where \\(Q,K,V\\) are the matrices containing the query, key and value vectors of every token in the text as rows. \\(d_k\\) denotes the size of the key vector, and the normalization ensures the model is numerically stable. Without this normalization, even if \\(Q\\) and \\(K\\) have unit variance, the variance of their product is of the order of the head size \\(d_k\\).\nSo far, we have not mentioned how to get the queries, keys and values from our tokens. We can choose any differentiable function. In the GPT architecture, they use a dense linear layer.\n\nclass AttentionHead(nn.Module):\n    \"Self-attention head.\"\n    def __init__(self, emb_sz, head_sz):\n        super().__init__()\n        # Define key, query and value (bias=False)\n        \n    def forward(self, x):\n        # Compute the attention\n        pass\n\nThis attention mechanism on its own, allows all the tokens to “see” each other at all times. This is what we would see in the transformer encoder, as all the source text in a translation task already exists. However, the transformer decoder can only attend to text as it is being generated. This means that, while we train it, we need to ensure that tokens cannot attend to what would be future innexistent ones.\nThis seems obvious because, at inference time, we clearly only have the text that is already generated. Nonetheless, during training, we sample full sequence chunks of a fixed sequence length. We can take the maximum advantage of this by training our model to generate new tokens for all the possible contexts available in this chunk, from a single initial token to all.\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 1, 8 \nxb, yb = get_batch(data_train, batch_size, seq_len)\n\n\nclass CausalAttentionHead(nn.Module):\n    \"Masked self-attention head.\"\n    def __init__(self, emb_sz, head_sz, seq_len, dropout=0.2):\n        super().__init__()\n        self.key = nn.Linear(emb_sz, head_sz, bias=False)\n        self.query = nn.Linear(emb_sz, head_sz, bias=False)\n        self.value = nn.Linear(emb_sz, head_sz, bias=False)\n        # Define the lower triangular matrix\n        \n            \n    def forward(self, x):\n        q = self.query(x) # (BS, T, H)\n        k = self.key(x)\n        v = self.value(x)\n        \n        _, T, _ = x.shape\n        w = q @ k.transpose(-2, -1) * k.shape[-1]**(-0.5)        # (BS, T, T)\n        # Apply the causal mask\n        \n        return self.dropout(w.softmax(-1)) @ v # (BS, T, H)\n\n\n\n\n\n\n\nNote\n\n\n\nThis implementation works well. However, pytorch provides a torch.nn.functional.scaled_dot_product_attention that uses specialized CUDA kernels.\n\n\nNow that we have implemented the self-attention attention mechanism, let’s make a first version of our GPT model. The model will have an embedding, an attention layer and a fully connected layer.\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, head_sz, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        # Define the embedding, attention and fully connected layer\n        \n    def forward(self, x):\n        # Compute the logits\n        pass\n    \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:] # Limit to maximum seq_len tokens\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, head_sz = 16, 16\ngpt = GPT(vocab_size, emb_sz, head_sz, seq_len).to(device)\n\n\ntrain_model(400, gpt, 1e-3, batch_size, seq_len)\n\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\nAlready with this basic transformer decoder, we reach a loss that is lower than the bigram model, but it is still not completing the task appropiately. Let’s keep the work up!"
  },
  {
    "objectID": "tutorial.html#positional-encoding",
    "href": "tutorial.html#positional-encoding",
    "title": "Tutorial",
    "section": "Positional encoding",
    "text": "Positional encoding\nWith self-attention, our model can combine the information between all the tokens, but it has no notion about the relative distances between them. To solve this, we can provide our model with a positional encoding, as it is illustrated in the previous figure.\nThere are many different ways to provide the model with information about the token positions. In GPT, they use a positional embedding. This is the same as the vocabulary embedding with the difference that we will have as many rows in the embedding matrix as the maximum sequence length that we allow our model to process.\nLet’s implement it!\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, head_sz, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        # Define the positional embedding\n        \n        self.attn = CausalAttentionHead(emb_sz, head_sz, seq_len)\n        self.linear = nn.Linear(head_sz, vocab_sz)\n        \n    def forward(self, x):\n        # Compute the logits adding the token and positional embeddings\n        pass\n    \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, head_sz = 16, 16\ngpt = GPT(vocab_size, emb_sz, head_sz, seq_len).to(device)\n\n\ntrain_model(1000, gpt, 5e-4, batch_size, seq_len)\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\nWe have significantly reduced the loss, but it actually seems to do worse!"
  },
  {
    "objectID": "tutorial.html#multi-head-attention",
    "href": "tutorial.html#multi-head-attention",
    "title": "Tutorial",
    "section": "Multi-head attention",
    "text": "Multi-head attention\nSo far, we have only used a single attention head in our model. In the GPT architecture, we use multi-head attention which consists of running various independent. Then, we concatenate the output of the different heads and project the resulting feature vectors to the original embedding size.\n\nclass MultiHeadAttention(nn.Module):\n    \"Multiple parallel self-attention heads.\"\n\n    def __init__(self, num_heads, emb_sz, head_sz, seq_len):\n        super().__init__()\n        # Define the heads in an nn.ModuleList and the projector linear layer\n\n    def forward(self, x):\n        # Concatenate the head outputs and project it to the embedding dimension\n        pass\n\nUsually, we take the embedding size and divide it by the number of heads to have better control of the matrix sizes within our model.\nHere, we have implemented the heads sequentially instead of in parallel. There is a much faster way to compute all the attention heads at once. The tensor dimensions in the self-attention module are [BS, T, E], where E denotes the embedding size. Since all the opperations are carried over the last two dimensions, if we reshape the Q, K, V tensors to [BS, NH, T, HS], where NH and HS denote the number of heads and head size, respectively, we can compute the self-attention for all the heads at once.\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, n_head, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        self.embedding_pos = nn.Embedding(seq_len, emb_sz)\n        # Define the head size and the multi-head attention\n        \n        self.linear = nn.Linear(emb_sz, vocab_sz)\n        \n    def forward(self, x):\n        emb_tkn = self.embedding_tkn(x)\n        emb_pos = self.embedding_pos(torch.arange(x.shape[1], device=x.device))\n        x = emb_tkn + emb_pos\n        x = self.attn(x)\n        return self.linear(x)\n    \n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_head = 16, 2\ngpt = GPT(vocab_size, emb_sz, n_head, seq_len).to(device)\n\n\ngpt = train_model(5000, gpt, 2e-4, batch_size, seq_len)\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())"
  },
  {
    "objectID": "tutorial.html#feedforward",
    "href": "tutorial.html#feedforward",
    "title": "Tutorial",
    "section": "Feedforward",
    "text": "Feedforward\nIn the transformer architecture, we find multi-head attention layers that are followed by feedforward parts. These two main parts constitute the main body of a repeating block that we can then stack several times.\nWith the self-attention, we had tokens exchanging information. With the feedforward part, we let the tokens elaborate on this information.\nLet’s implement the feedforward bit of the network. It is a multi-layer perceptron with a single hidden layer.\n\nclass FeedForward(nn.Module):\n    def __init__(self, emb_sz, dropout=0.2):\n        super().__init__()\n        # Define the two layers with GELU activation function\n\n    def forward(self, x):\n        # Apply the layers\n        pass"
  },
  {
    "objectID": "tutorial.html#decoder-block",
    "href": "tutorial.html#decoder-block",
    "title": "Tutorial",
    "section": "Decoder block",
    "text": "Decoder block\nWe grow our network by stacking decoder blocks. These have an initial self-attention part followed by a feedforward part. Concatenating blocks, we alternate between both, resulting in a combination of token communication and local computation.\nThere are two main key elements in the decoder block that we have not implemented yet. These are the residual paths and the layer normalization.\n\n\n\n\n\n\nNote\n\n\n\nBeware that in the GPT architecture, the normalization layers go before the self-attention and feedforward layers. This is an enhancement with respect to the original transformer architecture from ?@fig-transformer.\n\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, emb_sz, n_heads, seq_len):\n        super().__init__()\n        # Initial layer norm\n        # Head size and multi-head attention\n        # Second layer norm\n        # Feedforward part\n        \n    def forward(self, x):\n        # Apply the layers with the residual paths\n        pass\n\nNow we can rewrite our GPT models stacking a few blocks together.\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, n_blocks, n_heads, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        self.embedding_pos = nn.Embedding(seq_len, emb_sz)\n        # Define the blocks and the last layer norm\n\n        self.linear = nn.Linear(emb_sz, vocab_sz)\n        \n    def forward(self, x):\n        # Apply the whole architecture!\n        pass\n    \n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_blocks, n_heads = 16, 3, 2\ngpt = GPT(vocab_size, emb_sz, n_blocks, n_heads, seq_len).to(device)\n\n\ngpt = train_model(5000, gpt, 2e-4, batch_size, seq_len)\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\nTechnically, this generation is not wrong. Let’s provide it with a bit of extra context.\n\ncontext = torch.tensor([[3, 4, 4, 4, 0, 3, 4, 4, 5, 0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 40).tolist()[0])\n\nIt’s not perfect, but we’re getting there."
  },
  {
    "objectID": "tutorial.html#go-big-or-go-home",
    "href": "tutorial.html#go-big-or-go-home",
    "title": "Tutorial",
    "section": "Go big or go home",
    "text": "Go big or go home\nLet’s see how far we can push the model. As we grow the newtork, it is essential that we add some regularization, such as dropout.\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_blocks, n_heads = 64, 4, 8\ngpt = GPT(vocab_size, emb_sz, n_blocks, n_heads, seq_len).to(device)\n\n\ngpt = train_model(10000, gpt, 1e-4, batch_size, seq_len)\n\nStep 0: train loss 2.5046, val loss 2.4860\nStep 200: train loss 2.1752, val loss 2.4264\nStep 400: train loss 2.0528, val loss 2.3285\nStep 600: train loss 1.9620, val loss 2.2053\nStep 800: train loss 1.6613, val loss 1.9140\nStep 1000: train loss 1.2678, val loss 1.3279\nStep 1200: train loss 1.0419, val loss 0.9195\nStep 1400: train loss 0.9266, val loss 0.7967\nStep 1600: train loss 0.8495, val loss 0.7225\nStep 1800: train loss 0.8086, val loss 0.6540\nStep 2000: train loss 0.7846, val loss 0.6082\nStep 2200: train loss 0.7407, val loss 0.5484\nStep 2400: train loss 0.7069, val loss 0.5276\nStep 2600: train loss 0.6790, val loss 0.5213\nStep 2800: train loss 0.6550, val loss 0.4604\nStep 3000: train loss 0.6206, val loss 0.4564\nStep 3200: train loss 0.6230, val loss 0.4313\nStep 3400: train loss 0.5819, val loss 0.4089\nStep 3600: train loss 0.5919, val loss 0.3990\nStep 3800: train loss 0.5422, val loss 0.3756\nStep 4000: train loss 0.5757, val loss 0.3539\nStep 4200: train loss 0.5493, val loss 0.3613\nStep 4400: train loss 0.5248, val loss 0.3461\nStep 4600: train loss 0.5180, val loss 0.3421\nStep 4800: train loss 0.5198, val loss 0.3184\nStep 5000: train loss 0.4806, val loss 0.3184\nStep 5200: train loss 0.4996, val loss 0.3353\nStep 5400: train loss 0.5133, val loss 0.3156\nStep 5600: train loss 0.4976, val loss 0.3038\nStep 5800: train loss 0.5066, val loss 0.3003\nStep 6000: train loss 0.4901, val loss 0.2954\nStep 6200: train loss 0.4883, val loss 0.2951\nStep 6400: train loss 0.4717, val loss 0.2944\nStep 6600: train loss 0.4752, val loss 0.2763\nStep 6800: train loss 0.4771, val loss 0.2869\nStep 7000: train loss 0.4656, val loss 0.2769\nStep 7200: train loss 0.4768, val loss 0.2656\nStep 7400: train loss 0.4678, val loss 0.2896\nStep 7600: train loss 0.4505, val loss 0.2976\nStep 7800: train loss 0.4683, val loss 0.2885\nStep 8000: train loss 0.4828, val loss 0.2718\nStep 8200: train loss 0.4449, val loss 0.2778\nStep 8400: train loss 0.4472, val loss 0.2672\nStep 8600: train loss 0.4702, val loss 0.2790\nStep 8800: train loss 0.4432, val loss 0.2778\nStep 9000: train loss 0.4936, val loss 0.2839\nStep 9200: train loss 0.4809, val loss 0.2610\nStep 9400: train loss 0.4890, val loss 0.2844\nStep 9600: train loss 0.4797, val loss 0.2951\nStep 9800: train loss 0.4548, val loss 0.2792\nStep 9999: train loss 0.4566, val loss 0.2632\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'0,383421,383422,38342'\n\n\nThis model seems to know what it’s doing. Let’s try with a different context.\n\ncontext = torch.tensor([[5, 5, 0, 4, 9, 5, 6]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'44,384545,384546,384547,384'\n\n\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n',853803,853804,853805'\n\n\nPromising. Let’s see more!\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 80)[0].tolist())\n\n',686570,686571,686572,686574,686575,686576,686577,686577,686578,686579,686580,686'\n\n\nIn this sequence we see a couple of artifacts: it skips the 686573 and it repeats the 686577. However, it has learned how to change from 79 to 80. Let’s try again.\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 80)[0].tolist())\n\n',149120,149121,149122,149123,149124,149125,149126,149127,149128,149129,149130,149'\n\n\nFlawless. This model rocks!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPTutorial",
    "section": "",
    "text": "This tutorial has been developed for the VI Pyrenees winter school (Feb. 2023) of quantum information.\nIf you’re a participant of the school, please see material below.\nIf you’re not a participant of the school, you’re probably seeing this content after the actual hands-on tutorial has happened. However, you may find it useful to visit the the tutorial page and read through the completed tutorial walkthrough to learn about unsupervised learning and how to train a GPT-like architecture.\n\nContent overview\nFirst, we start introducing the basics of unsupervised learning and generative models. We explain the main concepts behind language models and how to train them, such as the tokenization process or the chain rule of probability.\n\n\n\n\n\n\nNote\n\n\n\nTo prepare this tutorial, we have used some of the content from our machine learning course.\n\n\nThen, we build a bigram language model, which allows us to introduce the concept of an embedding table and the typical training loops. This model serves both as baseline and warm up.\nFinally, we proceed to learn about the specific inner workings of GPT. We start by introducing the transformer architecture and the self-attention mechanism. Then, we show how to use causal self-attention (or masked self-attention) to train our model to generate new text. Finally, we dive into the details such as skip connections and layer normalization.\nTo finish, we train a larger model that sucessfully learns to count! :)\n\n\nMaterial\nWe provide two main notebooks that can run in colab:\n\nnbs/tutorial.ipynb is ready to be filled during the session\nnbs/metatutorial.ipynb is the already complete tutorial\n\nTo follow along with the tutorial, you need the first notebook. Ideally, you could simply go to its page and run it on google colab. However, since the wifi here is unstable, you can clone (or simply download) the repository to your computer.\nIf you want to follow along, make sure you have pytorch installed. We won’t use any fancy stuff from pytorch so you can simply run conda install pytorch -c pytorch to install it with conda or pip install torch to install it with pip. In case you have a GPU, find your installed CUDA version (e.g. 11.7) to install conda install pytorch-cuda=11.7 -c nvidia. Follow their getting started page in case of doubt."
  },
  {
    "objectID": "metatutorial.html#generative-modeling",
    "href": "metatutorial.html#generative-modeling",
    "title": "Tutorial walkthrough",
    "section": "Generative modeling",
    "text": "Generative modeling\nIn this tutorial, we focus on generative learning. As we have briefly mentioned before, it consists on learning the data distribution to generate new samples. This is extremely powerful both on its own, since high-quality new samples can be very valuable, and in combination with other tools to tackle downstream tasks.\nThere are many data generation approaches that we can consider. The most straightforward one is to simply generate samples that are similar to the traning ones, such as face images or digits (e.g., this cat). We can also have conditioned synthesis, such as generating an audio signal from a text prompt that can be conditioned to a specific speaker voice (e.g. WaveNet). This involves all sorts of translation tasks, where we write text from a sample fragment, generate a new image from a reference one (see the emblematic horse-to-zebra example), or even create a video from a text fragment!\n\n\n\n\n\n\nNote\n\n\n\nThis is a very broad field and here we just show a hand full of representative examples."
  },
  {
    "objectID": "metatutorial.html#example-task",
    "href": "metatutorial.html#example-task",
    "title": "Tutorial walkthrough",
    "section": "Example task",
    "text": "Example task\nThe goal of this tutorial is to train a GPT-like model to count numbers: “1,2,3,4,…,8765,8766,8767,…”. This seems like a rather simple task that could be easily achieved numerically with a single line of code. However, we will consider the digits as strings that conform sentences.\nThis toy example will allow us to understand the main concepts behind language models. We will use it as a running example and implement the main ideas as we see them.\nHere, we will build our data set, which is nothing more than a text document containing the numbers.\n\nmax_num = 1_000_000\ntext = \",\".join([str(i) for i in range(max_num)])\n\nLet’s see the first and last few digits of our data set.\n\n\nCode\nprint(text[:20])\nprint(text[-20:])\n\n\n0,1,2,3,4,5,6,7,8,9,\n999997,999998,999999"
  },
  {
    "objectID": "metatutorial.html#giving-numerical-meaning-to-text",
    "href": "metatutorial.html#giving-numerical-meaning-to-text",
    "title": "Tutorial walkthrough",
    "section": "Giving numerical meaning to text",
    "text": "Giving numerical meaning to text\nWe can communicate very deep concepts with words, but how does a machine understand them?\nWhen we work with text, we split it into elementary pieces called tokens. This is known as tokenization and there is quite a lot of freedom on how to do it. For example, we can take from full sentences, to words, to single characters. The most common practice is to use sub-word tokens that are between single characters to full words, such as SentencePiece. We can also have special tokens to account for additional grammatical information. For example, we can use special tokens to indicate the beggining and ending of a sequence, or to indicate that the words start with capital letters.\nLet’s see a simple tokenization example. We would take the following sentence:\nMy cat won't stop purring.\nAnd transform it into the tokens:\n&lt;BoS&gt;&lt;x_maj&gt;&lt;my&gt; &lt;cat&gt; &lt;wo&gt;&lt;n't&gt; &lt;stop&gt; &lt;purr&gt;&lt;ing&gt;&lt;.&gt;&lt;EoS&gt;\n\n\n\n\n\n\nNote\n\n\n\nI just made up this tokenization, this is just to provide an idea.\n\n\nWith this, we define a vocabulary of tokens. To provide them with “meaning”, we assign a trainable parameter vector to each of them, which are known as embedding vectors. The larger the embedding, the richer the information we can associate to every individual token. We typically store these vectors in a so-called embedding matrix, where every row provides the associated embedding vector to a token. This way, we identify the tokens by an integer index that corresponds to their row in the embedding matrix.\nTaking long tokens results into large vocabularies and, therefore, we need more memory. However, we can generate a piece of text with just a few inference steps. Conversely, short tokens require much less memory at the cost of more inference steps to write. Thus, this presents a trade-off between memory and computational time. You can get some intuition about it by comparing the number of letters in the alphabet (shortest possible tokens) with the number of entries in a dictionary (every word is a token).\nTo process a piece of text, we first split it into the tokens of our vocabulary (tokenization), and replace the tokens by their corresponding indices (numericalization).\nLet’s see how this works in our example task. First of all, we build the token vocabulary. In this simple case, every digit is a token together with the separator “,”.\n\nvocab = sorted(list(set(text)))\nvocab_size = len(vocab)\nvocab\n\n[',', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n\nNow we can build a Tokenizer class to encode raw text into tokens, and decode tokens to actual text.\n\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.s2i = {char: i for i, char in enumerate(vocab)}\n        self.i2s = {i: char for char, i in self.s2i.items()}\n    \n    def encode(self, string):\n        \"Tokenize an input string\"\n        return [self.s2i[char] for char in string]\n    \n    def decode(self, ints):\n        \"Transform a list of integers to a string of characters\"\n        return ''.join([self.i2s[i] for i in ints])\n\n\ntkn = Tokenizer(vocab)\n\nLet’s see the map from tokens to integer.\n\ntkn.s2i\n\n{',': 0,\n '0': 1,\n '1': 2,\n '2': 3,\n '3': 4,\n '4': 5,\n '5': 6,\n '6': 7,\n '7': 8,\n '8': 9,\n '9': 10}\n\n\nWe can try our tokenizer with a text example.\n\npre_tkn = text[:10]\npre_tkn, tkn.encode(pre_tkn)\n\n('0,1,2,3,4,', [1, 0, 2, 0, 3, 0, 4, 0, 5, 0])\n\n\nWe can also test the decoding function by encoding and decoding.\n\ntkn.decode(tkn.encode(pre_tkn))\n\n'0,1,2,3,4,'\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere we only perform the text pre-processing. The embedding belongs to the machine learning model."
  },
  {
    "objectID": "metatutorial.html#learning-the-data-probability-distribution",
    "href": "metatutorial.html#learning-the-data-probability-distribution",
    "title": "Tutorial walkthrough",
    "section": "Learning the data probability distribution",
    "text": "Learning the data probability distribution\nTo learn how to generate text, we need to learn the underlying distribution of the data we wish to replicate \\(p_{\\text{data}}(\\mathbf{x})\\). We model text as a sequence of tokens \\(\\mathbf{x}=\\left[x_1, x_2, \\dots, x_{T-1}\\right]\\), and the goal is to predict the next token \\(x_T\\). This way, we can recursively generate text:\n\nWe start with some initial context \\(x_1, x_2, \\dots, x_{T-1}\\).\nWe predict the next token \\(x_T\\), given the context.\nWe append the prediction to the existing text and repeat the process taking \\(x_1,\\dots,x_T\\) as context.\n\nWe typically do this defining a parametrized model to approximate the probability distribution, \\(p_\\theta(\\mathbf{x})\\approx p_{\\text{data}}(\\mathbf{x})\\). The parameters \\(\\theta\\) can represent from the weights of a neural network, to the coefficients of a gaussian mixture model.\nA standard technique in the machine learning field is to use the chain rule of probability to model sequential data. This way, the probability to observe a sequence of tokens can be described as \\[p_{\\theta}(\\mathbf{x})=p_\\theta(x_1)\\prod_{t=2}^{T}p_\\theta(x_t|x_1\\dots x_{t-1})\\,.\\]\nWe optimize our model parameters to obtain the maximum likelihood estimator, which is the most statistically efficient estimator. In this tutorial, we do not want to dive too deep in the details. The main intuition behind it is that we try to maximize the likelihood of observing the training data under our parametrized model. As such, we wish to minimize the negative log-likelihood loss or cross-entropy loss: \\[\\theta^* = \\text{arg}\\,\\text{min}_\\theta - \\frac{1}{N}\\sum_{i=1}^N \\log p_\\theta\\left(\\mathbf{x}^{(i)}\\right) = \\text{arg}\\,\\text{min}_\\theta - \\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^T \\log p_\\theta\\left(x_t^{(i)}|x_{&lt;t}^{(i)}\\right)\\]\nWe can understand the task as a classification problem at every time-step where the goal is to predict the token that follows. Thus, we can build our self-supervised classification task by simply taking the text shifted by one position as target for our prediction. For example, consider the tokenized sentence\n&lt;this&gt; &lt;language&gt; &lt;model&gt; &lt;rocks&gt;&lt;!&gt;\nGiven the tokens\n&lt;this&gt; &lt;language&gt;\nwe wish to predict\n&lt;model&gt;\namong all the tokens in the vocabulary.\nAs we typically do in machine learning, we find the optimal parameters \\(\\theta^*\\), i.e., train our model, with gradient-based optimization."
  },
  {
    "objectID": "metatutorial.html#data-processing",
    "href": "metatutorial.html#data-processing",
    "title": "Tutorial walkthrough",
    "section": "Data processing",
    "text": "Data processing\nFirst of all, we need to properly arrange our data. We will start by tokenizing the whole text piece.\n\ndata = torch.tensor(tkn.encode(text))\ndata[:20]\n\ntensor([ 1,  0,  2,  0,  3,  0,  4,  0,  5,  0,  6,  0,  7,  0,  8,  0,  9,  0,\n        10,  0])\n\n\nNow we need to save a part of the data for validation and keep the rest for training. In generative models, we do not tend to use too much data for validation because it is just to get a rough idea of how it is working. In the end, we will evaluate the performance ourselves asking the model to generate samples.\nTo keep this simple, we will save the last numbers as validation data.\n\n\n\n\n\n\nNote\n\n\n\nGiven the nature of our data, it would be best to save chunks of the data sampled at different points along the whole text piece.\n\n\n\nval_pct = 0.1\nsplit_idx = int(len(data)*val_pct)\ndata_train = data[:-split_idx]\ndata_val = data[-split_idx:]\n\n\ndata_train.shape, data_val.shape\n\n(torch.Size([6200001]), torch.Size([688888]))\n\n\nTo train machine learning models, we take advantage of parallelization to process several samples at once. To do so, we will split the text in sub-sequences from which we will build our training batches.\n\ndef get_batch(data, batch_size, seq_len):\n    idx = torch.randint(len(data)-seq_len, (batch_size,))\n    x = torch.stack([data[i:i+seq_len] for i in idx])\n    y = torch.stack([data[i:i+seq_len] for i in idx+1])\n    return x.to(device), y.to(device)\n\n\nbatch_size = 64\nseq_len = 8\nxb, yb = get_batch(data_train, batch_size, seq_len)\n\n\nxb.shape\n\ntorch.Size([64, 8])\n\n\n\nxb[0], yb[0]\n\n(tensor([ 3,  0,  8,  8, 10,  1,  2,  4], device='cuda:0'),\n tensor([ 0,  8,  8, 10,  1,  2,  4,  0], device='cuda:0'))"
  },
  {
    "objectID": "metatutorial.html#model-definition",
    "href": "metatutorial.html#model-definition",
    "title": "Tutorial walkthrough",
    "section": "Model definition",
    "text": "Model definition\nWe will make a bigram model that predicts the following character based on the previous one. These models are stochastic and, therefore, the output of the model is a probability distribution over our vocabulary. We can easily achieve this by making the embedding size as large as the vocabulary. This way, when we index into the embedding matrix with a token, we immediately obtain the probability distribution over the possible next tokens.\n\nclass BigramLanguageModel(nn.Module):\n    \"Language model that predicts text based on the previous character.\"\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, x):\n        logits = self.embedding(x)\n        return logits\n        \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            logits = self(x)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\nbigram_model = BigramLanguageModel(vocab_size).to(device)\n\n\nxb.shape, bigram_model(xb).shape\n\n(torch.Size([64, 8]), torch.Size([64, 8, 11]))\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe logits we define here are the unnormalized probability scores for each token. To transform them in a normalized probability distribution, we use a SoftMax function. We will see below that pytorch takes the logits directly to compute the loss function instead of the probabilities.\n\n\nLet’s try generating some text with our model.\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(bigram_model.generate(context, 20)[0].tolist())\n\n',,779031473,20590,877'"
  },
  {
    "objectID": "metatutorial.html#training-loop",
    "href": "metatutorial.html#training-loop",
    "title": "Tutorial walkthrough",
    "section": "Training loop",
    "text": "Training loop\nWith the data and the model, we’re almost ready to do the training. We need to define a loss function and an optimiziation algorithm to update our model parameters.\nAs we have mentioned before, we wish to minimize the negative log-likelihood of the data with respect to the model. To do so, we use pytorch’s cross entropy loss.\n\ndef cross_entropy_loss(logits, targets):\n    \"Cross entropy loss flattening tensors\"\n    BS, T, H = logits.shape\n    loss = F.cross_entropy(logits.view(BS*T, H), targets.view(-1))\n    return loss\n\nThen, as optimizer, we will use Adam.\n\noptimizer = torch.optim.AdamW(bigram_model.parameters(), lr=1e-3)\n\nNow let’s define the training loop.\n\nbatch_size = 32\nseq_len = 24\ntrain_steps = 3000\n\nfor _ in range(train_steps):\n    xb, yb = get_batch(data_train, batch_size, seq_len)\n    \n    optimizer.zero_grad()\n    logits = bigram_model(xb)\n    loss = cross_entropy_loss(logits, yb)\n    loss.backward()\n    optimizer.step()\n    \nprint(loss.item())\n\n2.370857000350952\n\n\nWe will plug this into a function for later usage and estimate the loss on the validation set.\n\ndef train_model(steps, model, lr, batch_sz, seq_len):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for i in range(steps):\n        xb, yb = get_batch(data_train, batch_sz, seq_len)\n        optimizer.zero_grad()\n        logits = model(xb)\n        loss = cross_entropy_loss(logits, yb)\n        loss.backward()\n        optimizer.step()\n        if i % 200 == 0 or i == steps - 1:\n            losses = estimate_loss(model, batch_sz, seq_len)\n            print(f\"Step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    return model\n\n@torch.no_grad()\ndef estimate_loss(model, batch_sz, seq_len, eval_iters=50):\n    \"\"\"Estimate losses for train and validation data sets.\n    Adapted from https://github.com/karpathy/nanoGPT\"\"\"\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(data_train if split == 'train' else data_val,\n                             batch_sz, seq_len)\n            logits = model(X)\n            loss = cross_entropy_loss(logits, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nBigram models can’t accomplish this example task. After every digit, all the other digits are equally likely to happen if we do not consider any further context. This model can only take advantage of the separator ,. For instance, we know there will not be two consecutive separators and that the following number won’t start with 0.\nWe can see this in the first row of the embedding matrix.\n\n\nCode\nembedding_matrix = list(bigram_model.parameters())[0] \nembedding_matrix.softmax(-1)[0]\n\n\ntensor([0.0007, 0.0007, 0.1393, 0.0884, 0.1212, 0.1808, 0.1448, 0.1381, 0.0857,\n        0.0989, 0.0012], device='cuda:0', grad_fn=&lt;SelectBackward0&gt;)\n\n\nLet’s generate some text.\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(bigram_model.generate(context, 20)[0].tolist())\n\n',69,475,8423,1207856,'\n\n\nIn contrast to the previous example, we see the model has learned to not add consecutive separators, but the digits are still random. GPT time!"
  },
  {
    "objectID": "metatutorial.html#transformer",
    "href": "metatutorial.html#transformer",
    "title": "Tutorial walkthrough",
    "section": "Transformer",
    "text": "Transformer\nThe architecture behind the GPT language models is based on the transformer, depicted in Figure 3.\n\n\n\nFigure 3: Transformer schematic representation.\n\n\nThe transformer was introduced as an architecture for translation tasks with two main parts: the encoder (left) and the decoder (right). The decoder is the responsible part for generating the translated text and, thus, it is the language model bit of the whole architecture.\nThe transformer architecture relies heavily on self-attention mechanisms. Indeed, the original paper is called “Attention is all you need”. Unlike the bigram model, the transformer decoder can account for all the possible relationships between tokens in the past text to generate the new tokens."
  },
  {
    "objectID": "metatutorial.html#causal-self-attention",
    "href": "metatutorial.html#causal-self-attention",
    "title": "Tutorial walkthrough",
    "section": "Causal self-attention",
    "text": "Causal self-attention\nThe key element in the transformer architecture is the self-attention layer. This allows our tokens in our text piece to “communicate with each other” in a fixed way:\n\nFor every token, we compute three quantities: a key \\(\\mathbf{k}\\), a query \\(\\mathbf{q}\\) and a value \\(\\mathbf{v}\\).\nThen, tokens compare their query to the other tokens’ keys.\nThe resulting value for each token is the weighted average of all the values according to the query-key similarity.\n\nWe compute the similarity between keys and queries doing the dot product between the vectors. Then, to ensure the similarity weights are normalized, we apply the softmax activation function to all the dot products of the query of interest with all the keys. We can efficiently compute all of these with matrix multiplications: \\[\\text{Attention}(Q,K,V) = \\text{SoftMax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\,\\] where \\(Q,K,V\\) are the matrices containing the query, key and value vectors of every token in the text as rows. \\(d_k\\) denotes the size of the key vector, and the normalization ensures the model is numerically stable. Without this normalization, even if \\(Q\\) and \\(K\\) have unit variance, the variance of their product is of the order of the head size \\(d_k\\).\nSo far, we have not mentioned how to get the queries, keys and values from our tokens. We can choose any differentiable function. In the GPT architecture, they use a dense linear layer.\n\nclass AttentionHead(nn.Module):\n    \"Self-attention head.\"\n    def __init__(self, emb_sz, head_sz):\n        super().__init__()\n        self.key = nn.Linear(emb_sz, head_sz, bias=False)\n        self.query = nn.Linear(emb_sz, head_sz, bias=False)\n        self.value = nn.Linear(emb_sz, head_sz, bias=False)\n    \n    def forward(self, x):\n        q = self.query(x) # (BS, T, H)\n        k = self.key(x)\n        v = self.value(x)\n        \n        w = q @ k.transpose(-2, -1) * k.shape[-1]**(-0.5) # (BS, T, T)\n        return w.softmax(-1) @ v # (BS, T, H)\n\nThis attention mechanism on its own, allows all the tokens to “see” each other at all times. This is what we would see in the transformer encoder, as all the source text in a translation task already exists. However, the transformer decoder can only attend to text as it is being generated. This means that, while we train it, we need to ensure that tokens cannot attend to what would be future innexistent ones.\nThis seems obvious because, at inference time, we clearly only have the text that is already generated. Nonetheless, during training, we sample full sequence chunks of a fixed sequence length. We can take the maximum advantage of this by training our model to generate new tokens for all the possible contexts available in this chunk, from a single initial token to all.\n\nbatch_size, seq_len = 1, 8 \nxb, yb = get_batch(data_train, batch_size, seq_len)\n\n\nclass CausalAttentionHead(nn.Module):\n    \"Masked self-attention head.\"\n    def __init__(self, emb_sz, head_sz, seq_len, dropout=0.2):\n        super().__init__()\n        self.key = nn.Linear(emb_sz, head_sz, bias=False)\n        self.query = nn.Linear(emb_sz, head_sz, bias=False)\n        self.value = nn.Linear(emb_sz, head_sz, bias=False)\n        self.register_buffer('mask', torch.tril(torch.ones(seq_len, seq_len)))\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        q = self.query(x) # (BS, T, H)\n        k = self.key(x)\n        v = self.value(x)\n        \n        _, T, _ = x.shape\n        w = q @ k.transpose(-2, -1) * k.shape[-1]**(-0.5)    # (BS, T, T)\n        w = w.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n        return self.dropout(w.softmax(-1)) @ v # (BS, T, H)\n\n\n\n\n\n\n\nNote\n\n\n\nThis implementation works well. However, pytorch provides a torch.nn.functional.scaled_dot_product_attention that uses specialized CUDA kernels.\n\n\nNow that we have implemented the self-attention attention mechanism, let’s make a first version of our GPT model. The model will have an embedding, an attention layer and a fully connected layer.\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, head_sz, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding = nn.Embedding(vocab_sz, emb_sz)\n        self.attn = CausalAttentionHead(emb_sz, head_sz, seq_len)\n        self.linear = nn.Linear(head_sz, vocab_sz)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.attn(x)\n        return self.linear(x)\n    \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, head_sz = 16, 16\ngpt = GPT(vocab_size, emb_sz, head_sz, seq_len).to(device)\n\n\ntrain_model(400, gpt, 1e-3, batch_size, seq_len)\n\nStep 0: train loss 2.4356, val loss 2.4098\nStep 200: train loss 2.2399, val loss 2.3747\nStep 399: train loss 2.2106, val loss 2.3604\n\n\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n',9,56863316,5276562,5'\n\n\nAlready with this basic transformer decoder, we reach a loss that is lower than the bigram model, but it is still not completing the task appropiately. Let’s keep the work up!"
  },
  {
    "objectID": "metatutorial.html#positional-encoding",
    "href": "metatutorial.html#positional-encoding",
    "title": "Tutorial walkthrough",
    "section": "Positional encoding",
    "text": "Positional encoding\nWith self-attention, our model can combine the information between all the tokens, but it has no notion about the relative distances between them. To solve this, we can provide our model with a positional encoding, as it is illustrated in Figure 3.\nThere are many different ways to provide the model with information about the token positions. In GPT, they use a positional embedding. This is the same as the vocabulary embedding with the difference that we will have as many rows in the embedding matrix as the maximum sequence length that we allow our model to process.\nLet’s implement it!\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, head_sz, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        self.embedding_pos = nn.Embedding(seq_len, emb_sz)\n        self.attn = CausalAttentionHead(emb_sz, head_sz, seq_len)\n        self.linear = nn.Linear(head_sz, vocab_sz)\n        \n    def forward(self, x):\n        emb_tkn = self.embedding_tkn(x)\n        emb_pos = self.embedding_pos(torch.arange(x.shape[1], device=x.device))\n        x = emb_tkn + emb_pos\n        x = self.attn(x)\n        return self.linear(x)\n    \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, head_sz = 16, 16\ngpt = GPT(vocab_size, emb_sz, head_sz, seq_len).to(device)\n\n\ntrain_model(1000, gpt, 5e-4, batch_size, seq_len)\n\nStep 0: train loss 2.3120, val loss 2.0298\nStep 200: train loss 1.6010, val loss 1.3704\nStep 400: train loss 1.5358, val loss 1.3347\nStep 600: train loss 1.4745, val loss 1.2902\nStep 800: train loss 1.4576, val loss 1.2444\nStep 999: train loss 1.4536, val loss 1.2777\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'01100,,91100,,02101,,'\n\n\nWe have significantly reduced the loss, but it actually seems to do worse!"
  },
  {
    "objectID": "metatutorial.html#multi-head-attention",
    "href": "metatutorial.html#multi-head-attention",
    "title": "Tutorial walkthrough",
    "section": "Multi-head attention",
    "text": "Multi-head attention\nSo far, we have only used a single attention head in our model. In the GPT architecture, we use multi-head attention which consists of running various independent. Then, we concatenate the output of the different heads and project the resulting feature vectors to the original embedding size.\n\nclass MultiHeadAttention(nn.Module):\n    \"Multiple parallel self-attention heads.\"\n\n    def __init__(self, num_heads, emb_sz, head_sz, seq_len):\n        super().__init__()\n        self.heads = nn.ModuleList([CausalAttentionHead(emb_sz, head_sz, seq_len)\n                                    for _ in range(num_heads)])\n        self.linear = nn.Linear(head_sz*num_heads, emb_sz)\n\n    def forward(self, x):\n        x = torch.cat([head(x) for head in self.heads], dim=-1)\n        x = self.linear(x)\n        return x\n\nUsually, we take the embedding size and divide it by the number of heads to have better control of the matrix sizes within our model.\n\n\n\n\n\n\nNote\n\n\n\nHere, we have implemented the heads sequentially instead of in parallel. There is a much faster way to compute all the attention heads at once. The tensor dimensions in the self-attention module are [BS, T, E], where E denotes the embedding size. Since all the opperations are carried over the last two dimensions, if we reshape the Q, K, V tensors to [BS, NH, T, HS], where NH and HS denote the number of heads and head size, respectively, we can compute the self-attention for all the heads at once.\n\n\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, n_head, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        self.embedding_pos = nn.Embedding(seq_len, emb_sz)\n        head_sz = emb_sz // n_head\n        self.attn = MultiHeadAttention(n_head, emb_sz, head_sz, seq_len)\n        self.linear = nn.Linear(emb_sz, vocab_sz)\n        \n    def forward(self, x):\n        emb_tkn = self.embedding_tkn(x)\n        emb_pos = self.embedding_pos(torch.arange(x.shape[1], device=x.device))\n        x = emb_tkn + emb_pos\n        x = self.attn(x)\n        return self.linear(x)\n    \n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_head = 16, 2\ngpt = GPT(vocab_size, emb_sz, n_head, seq_len).to(device)\n\n\ngpt = train_model(5000, gpt, 2e-4, batch_size, seq_len)\n\nStep 0: train loss 2.4114, val loss 2.4379\nStep 200: train loss 2.3326, val loss 2.3935\nStep 400: train loss 2.2342, val loss 2.3961\nStep 600: train loss 2.1180, val loss 2.3374\nStep 800: train loss 1.9681, val loss 2.1082\nStep 1000: train loss 1.7882, val loss 1.8349\nStep 1200: train loss 1.5916, val loss 1.5813\nStep 1400: train loss 1.4464, val loss 1.3999\nStep 1600: train loss 1.3529, val loss 1.2617\nStep 1800: train loss 1.3099, val loss 1.1754\nStep 2000: train loss 1.2533, val loss 1.1168\nStep 2200: train loss 1.2490, val loss 1.0802\nStep 2400: train loss 1.2136, val loss 1.0455\nStep 2600: train loss 1.2242, val loss 1.0329\nStep 2800: train loss 1.2253, val loss 1.1018\nStep 3000: train loss 1.2024, val loss 0.9837\nStep 3200: train loss 1.1999, val loss 1.0419\nStep 3400: train loss 1.2010, val loss 1.0247\nStep 3600: train loss 1.2008, val loss 0.9965\nStep 3800: train loss 1.1920, val loss 1.0156\nStep 4000: train loss 1.2110, val loss 0.9998\nStep 4200: train loss 1.1858, val loss 1.0006\nStep 4400: train loss 1.1929, val loss 1.0027\nStep 4600: train loss 1.1902, val loss 1.0175\nStep 4800: train loss 1.1692, val loss 0.9676\nStep 4999: train loss 1.1663, val loss 1.0324\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'081111218121121812,12'"
  },
  {
    "objectID": "metatutorial.html#feedforward",
    "href": "metatutorial.html#feedforward",
    "title": "Tutorial walkthrough",
    "section": "Feedforward",
    "text": "Feedforward\nIn the transformer architecture, we find multi-head attention layers that are followed by feedforward parts. These two main parts constitute the main body of a repeating block that we can then stack several times.\nWith the self-attention, we had tokens exchanging information. With the feedforward part, we let the tokens elaborate on this information.\nLet’s implement the feedforward bit of the network. It is a multi-layer perceptron with a single hidden layer.\n\nclass FeedForward(nn.Module):\n    def __init__(self, emb_sz, dropout=0.2):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Linear(emb_sz, 4*emb_sz),\n                                    nn.GELU(),\n                                    nn.Linear(4*emb_sz, emb_sz),\n                                    nn.Dropout(dropout))\n\n    def forward(self, x):\n        return self.layers(x)"
  },
  {
    "objectID": "metatutorial.html#decoder-block",
    "href": "metatutorial.html#decoder-block",
    "title": "Tutorial walkthrough",
    "section": "Decoder block",
    "text": "Decoder block\nWe grow our network by stacking decoder blocks. These have an initial self-attention part followed by a feedforward part. Concatenating blocks, we alternate between both, resulting in a combination of token communication and local computation.\nThere are two main key elements in the decoder block that we have not implemented yet. These are the residual paths and the layer normalization.\n\n\n\n\n\n\nNote\n\n\n\nBeware that in the GPT architecture, the normalization layers go before the self-attention and feedforward layers. This is an enhancement with respect to the original transformer architecture from Figure 3.\n\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, emb_sz, n_heads, seq_len):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(emb_sz)\n        head_sz = emb_sz // n_heads\n        self.heads = MultiHeadAttention(n_heads, emb_sz, head_sz, seq_len)\n        self.norm_2 = nn.LayerNorm(emb_sz)\n        self.ffw = FeedForward(emb_sz)\n        \n    def forward(self, x):\n        x = x + self.heads(self.norm_1(x))\n        x = x + self.ffw(self.norm_2(x))\n        return x\n\nNow we can rewrite our GPT models stacking a few blocks together.\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, n_blocks, n_heads, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        self.embedding_pos = nn.Embedding(seq_len, emb_sz)\n        self.blocks = nn.Sequential(*[DecoderBlock(emb_sz, n_heads, seq_len)\n                                      for _ in range(n_blocks)])\n        self.layer_norm = nn.LayerNorm(emb_sz)\n        self.linear = nn.Linear(emb_sz, vocab_sz)\n        \n    def forward(self, x):\n        emb_tkn = self.embedding_tkn(x)\n        emb_pos = self.embedding_pos(torch.arange(x.shape[1], device=x.device))\n        x = emb_tkn + emb_pos\n        x = self.blocks(x)\n        return self.linear(self.layer_norm(x))\n    \n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_blocks, n_heads = 16, 3, 2\ngpt = GPT(vocab_size, emb_sz, n_blocks, n_heads, seq_len).to(device)\n\n\ngpt = train_model(5000, gpt, 2e-4, batch_size, seq_len)\n\nStep 0: train loss 2.5447, val loss 2.5251\nStep 200: train loss 2.3506, val loss 2.4647\nStep 400: train loss 2.3077, val loss 2.5329\nStep 600: train loss 2.2680, val loss 2.5167\nStep 800: train loss 2.2413, val loss 2.5738\nStep 1000: train loss 2.2003, val loss 2.5733\nStep 1200: train loss 2.1812, val loss 2.5701\nStep 1400: train loss 2.1544, val loss 2.4687\nStep 1600: train loss 2.1205, val loss 2.5322\nStep 1800: train loss 2.0167, val loss 2.2486\nStep 2000: train loss 1.8641, val loss 2.1164\nStep 2200: train loss 1.6780, val loss 1.7356\nStep 2400: train loss 1.4727, val loss 1.4668\nStep 2600: train loss 1.3467, val loss 1.2514\nStep 2800: train loss 1.2467, val loss 1.1161\nStep 3000: train loss 1.2174, val loss 1.0631\nStep 3200: train loss 1.1694, val loss 1.0006\nStep 3400: train loss 1.1376, val loss 0.9736\nStep 3600: train loss 1.1015, val loss 0.9086\nStep 3800: train loss 1.0782, val loss 0.9290\nStep 4000: train loss 1.0529, val loss 0.8534\nStep 4200: train loss 1.0305, val loss 0.8555\nStep 4400: train loss 0.9912, val loss 0.8366\nStep 4600: train loss 0.9993, val loss 0.8055\nStep 4800: train loss 0.9802, val loss 0.8178\nStep 4999: train loss 0.9659, val loss 0.7985\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'050547561056756105675'\n\n\nTechnically, this generation is not wrong. Let’s provide it with a bit of extra context.\n\ncontext = torch.tensor([[3, 4, 4, 4, 0, 3, 4, 4, 5, 0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 40).tolist()[0])\n\n'2333,2334,3,236433,232433,232433,232477,236433,23,'\n\n\nIt’s not perfect, but we’re getting there."
  },
  {
    "objectID": "metatutorial.html#go-big-or-go-home",
    "href": "metatutorial.html#go-big-or-go-home",
    "title": "Tutorial walkthrough",
    "section": "Go big or go home",
    "text": "Go big or go home\nLet’s see how far we can push the model. As we grow the newtork, it is essential that we add some regularization, such as dropout.\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_blocks, n_heads = 64, 4, 8\ngpt = GPT(vocab_size, emb_sz, n_blocks, n_heads, seq_len).to(device)\n\n\ngpt = train_model(10000, gpt, 1e-4, batch_size, seq_len)\n\nStep 0: train loss 2.5046, val loss 2.4860\nStep 200: train loss 2.1752, val loss 2.4264\nStep 400: train loss 2.0528, val loss 2.3285\nStep 600: train loss 1.9620, val loss 2.2053\nStep 800: train loss 1.6613, val loss 1.9140\nStep 1000: train loss 1.2678, val loss 1.3279\nStep 1200: train loss 1.0419, val loss 0.9195\nStep 1400: train loss 0.9266, val loss 0.7967\nStep 1600: train loss 0.8495, val loss 0.7225\nStep 1800: train loss 0.8086, val loss 0.6540\nStep 2000: train loss 0.7846, val loss 0.6082\nStep 2200: train loss 0.7407, val loss 0.5484\nStep 2400: train loss 0.7069, val loss 0.5276\nStep 2600: train loss 0.6790, val loss 0.5213\nStep 2800: train loss 0.6550, val loss 0.4604\nStep 3000: train loss 0.6206, val loss 0.4564\nStep 3200: train loss 0.6230, val loss 0.4313\nStep 3400: train loss 0.5819, val loss 0.4089\nStep 3600: train loss 0.5919, val loss 0.3990\nStep 3800: train loss 0.5422, val loss 0.3756\nStep 4000: train loss 0.5757, val loss 0.3539\nStep 4200: train loss 0.5493, val loss 0.3613\nStep 4400: train loss 0.5248, val loss 0.3461\nStep 4600: train loss 0.5180, val loss 0.3421\nStep 4800: train loss 0.5198, val loss 0.3184\nStep 5000: train loss 0.4806, val loss 0.3184\nStep 5200: train loss 0.4996, val loss 0.3353\nStep 5400: train loss 0.5133, val loss 0.3156\nStep 5600: train loss 0.4976, val loss 0.3038\nStep 5800: train loss 0.5066, val loss 0.3003\nStep 6000: train loss 0.4901, val loss 0.2954\nStep 6200: train loss 0.4883, val loss 0.2951\nStep 6400: train loss 0.4717, val loss 0.2944\nStep 6600: train loss 0.4752, val loss 0.2763\nStep 6800: train loss 0.4771, val loss 0.2869\nStep 7000: train loss 0.4656, val loss 0.2769\nStep 7200: train loss 0.4768, val loss 0.2656\nStep 7400: train loss 0.4678, val loss 0.2896\nStep 7600: train loss 0.4505, val loss 0.2976\nStep 7800: train loss 0.4683, val loss 0.2885\nStep 8000: train loss 0.4828, val loss 0.2718\nStep 8200: train loss 0.4449, val loss 0.2778\nStep 8400: train loss 0.4472, val loss 0.2672\nStep 8600: train loss 0.4702, val loss 0.2790\nStep 8800: train loss 0.4432, val loss 0.2778\nStep 9000: train loss 0.4936, val loss 0.2839\nStep 9200: train loss 0.4809, val loss 0.2610\nStep 9400: train loss 0.4890, val loss 0.2844\nStep 9600: train loss 0.4797, val loss 0.2951\nStep 9800: train loss 0.4548, val loss 0.2792\nStep 9999: train loss 0.4566, val loss 0.2632\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'0,383421,383422,38342'\n\n\nThis model seems to know what it’s doing. Let’s try with a different context.\n\ncontext = torch.tensor([[5, 5, 0, 4, 9, 5, 6]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'44,384545,384546,384547,384'\n\n\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n',853803,853804,853805'\n\n\nPromising. Let’s see more!\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 80)[0].tolist())\n\n',686570,686571,686572,686574,686575,686576,686577,686577,686578,686579,686580,686'\n\n\nIn this sequence we see a couple of artifacts: it skips the 686573 and it repeats the 686577. However, it has learned how to change from 79 to 80. Let’s try again.\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 80)[0].tolist())\n\n',149120,149121,149122,149123,149124,149125,149126,149127,149128,149129,149130,149'\n\n\nFlawless. This model rocks!"
  }
]