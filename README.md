GPTutorial
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

This tutorial has been developed for the [VI Pyrenees winter
school](https://setcases6.wordpress.com/) (Feb.Â 2023) for quantum
information.

# Content overview

First, we start introducing the basics of unsupervised learning and
generative models. We explain the main concepts behind language models
and how to train them, such as the tokenization process or the chain
rule of probability. Then, we introduce the task, the data we will use
and we explain how we will process it.

<div>

> **Note**
>
> To prepare this tutorial, we have used some of the content from our
> [machine learning
> course](https://borjarequena.github.io/Neural-Network-Course/).

</div>

Then, we provide a brief introduction to [pytorch](https://pytorch.org/)
showing how to compute gradients with automatic differentiation and how
to build models. We build a bigram model as baseline to warm up.

Finally, we proceed to learn about the specific inner workings of GPT.
We start by introducing the transformer architecture and the
self-attention mechanism. Then, we show how to use masked self-attention
to train our model to generate new text. Finally, we dive into the
details such as skip connections and layer normalization.

<div>

> **Note**
>
> This is a live-coding tutorial. During the session, we go back and
> fourth to add layers of complexity to our models. Thus, the complete
> notebook may not be very informative. We recommend that you look at
> the MetaGPTutorial notebook, which is the guide we follow to do it
> during the session.

</div>
