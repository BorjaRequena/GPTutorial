GPTutorial
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

This tutorial has been developed for the [VI Pyrenees winter
school](https://setcases6.wordpress.com/) (Feb. 2023) of quantum
information.

If you’re not a participant of the school, you’re probably seeing this
content after the actual hands-on tutorial has happened. However, you
may find it useful to visit the [the tutorial
page](https://borjarequena.github.io/GPTutorial/) and read through the
completed [tutorial
walkthrough](https://borjarequena.github.io/GPTutorial/metatutorial.html)
to learn about unsupervised learning and how to train a GPT-like
architecture.

# Content overview

First, we start introducing the basics of unsupervised learning and
generative models. We explain the main concepts behind language models
and how to train them, such as the tokenization process or the chain
rule of probability.

<div>

> **Note**
>
> To prepare this tutorial, we have used some of the content from our
> [machine learning
> course](https://borjarequena.github.io/Neural-Network-Course/).

</div>

Then, we build a bigram language model, which allows us to introduce the
concept of an embedding table and the typical training loops. This model
serves both as baseline and warm up.

Finally, we proceed to learn about the specific inner workings of GPT.
We start by introducing the transformer architecture and the
self-attention mechanism. Then, we show how to use causal self-attention
(or masked self-attention) to train our model to generate new text.
Finally, we dive into the details such as skip connections and layer
normalization.

To finish, we train a larger model that sucessfully learns to count! :)

# Material

We provide two main notebooks that can run in colab:

- `nbs/tutorial.ipynb` is ready to be filled during the session
- `nbs/metatutorial.ipynb` is the already complete tutorial
